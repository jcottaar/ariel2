{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90a2345-b39e-4417-b97f-9254fd26d10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('/mnt/d/ariel2/code/core/')\n",
    "import kaggle_support as kgs\n",
    "import ariel_load\n",
    "import tqdm\n",
    "import copy\n",
    "import importlib\n",
    "import ariel_diagnostics\n",
    "train_data = kgs.load_all_train_data()\n",
    "kgs.sanity_checks_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcf2eb-724e-43e7-adb2-89201d69b173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████                                                                       | 124/1100 [02:50<29:11,  1.79s/it]"
     ]
    }
   ],
   "source": [
    "# FGS = []\n",
    "# AIRS = []\n",
    "# N_FGS=2500\n",
    "# N_AIRS=500\n",
    "# loaders = ariel_load.default_loaders()\n",
    "# for d in tqdm.tqdm(train_data):\n",
    "#     for t in d.transits:\n",
    "#         tt = copy.deepcopy(t)\n",
    "#         tt.load_to_step(2, d, loaders)\n",
    "#         for ii in range(tt.data[0].data.shape[0]//N_FGS):\n",
    "#             FGS.append(cp.mean(tt.data[0].data[N_FGS*ii:(N_FGS)*(ii+1),...],0).get())\n",
    "#         for ii in range(tt.data[1].data.shape[0]//N_AIRS):\n",
    "#             AIRS.append(cp.mean(tt.data[1].data[N_AIRS*ii:(N_AIRS)*(ii+1),...],0).get())\n",
    "# del tt\n",
    "# FGS = cp.array(FGS)\n",
    "# AIRS = cp.array(AIRS)\n",
    "# data = [FGS, AIRS]\n",
    "# kgs.dill_save(kgs.temp_dir + '/explore_sensor_modes.pickle', data)\n",
    "data = kgs.dill_load(kgs.temp_dir + '/explore_sensor_modes.pickle')\n",
    "print([d.shape for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d7f6f-39ba-48d9-a931-9c0c1b9658ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sens_id = 0\n",
    "plt.figure()\n",
    "plt.imshow(cp.log(cp.nanmean(data[sens_id],0)).get()/np.log(10))\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.plot(cp.nanmean(data[sens_id],axis=(1,2)).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76698c-7577-4640-98b7-3c267700475d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def nan_pca(mat, n_components):\n",
    "#     _, *rest = mat.shape\n",
    "#     print(rest)\n",
    "#     mat = copy.deepcopy(mat)\n",
    "#     mat = mat.reshape((mat.shape[0],-1))\n",
    "#     mat0 = copy.deepcopy(mat)\n",
    "#     mat0[:,cp.any(cp.isnan(mat0),0)] = 0\n",
    "#     u,s,components = cp.linalg.svd(mat0, full_matrices=False, compute_uv=True)\n",
    "#     weights = u*s  \n",
    "#     components = components[:n_components,:]\n",
    "#     weights = weights[:,:n_components]\n",
    "#     return weights, components\n",
    "\n",
    "# def nan_pca(mat, n_components, max_iter=50, tol=1e-6, verbose=False):\n",
    "#     \"\"\"\n",
    "#     Perform PCA on data with missing values using an EM-like iterative approach.\n",
    "\n",
    "#     Args:\n",
    "#         mat (cp.ndarray): Input data, shape (n_samples, ...). May contain NaNs.\n",
    "#         n_components (int): Number of principal components to retain.\n",
    "#         max_iter (int): Maximum number of iterations.\n",
    "#         tol (float): Convergence tolerance on reconstruction error change.\n",
    "#         verbose (bool): If True, prints iteration diagnostics.\n",
    "\n",
    "#     Returns:\n",
    "#         W (cp.ndarray): Projected scores (n_samples, n_components).\n",
    "#         C (cp.ndarray): Principal components (n_components, n_features).\n",
    "#         S (cp.ndarray): Singular values for each component (n_components,).\n",
    "#     \"\"\"\n",
    "#     # Flatten per-sample features\n",
    "#     orig_shape = mat.shape\n",
    "#     X = mat.copy()\n",
    "#     n_samples = X.shape[0]\n",
    "#     X = X.reshape(n_samples, -1)\n",
    "#     mask = ~cp.isnan(X)\n",
    "\n",
    "#     # Initialize missing entries to zero\n",
    "#     X_filled = X.copy()\n",
    "#     X_filled[~mask] = 0\n",
    "\n",
    "#     # Initial SVD\n",
    "#     U, S_full, Vt = cp.linalg.svd(X_filled, full_matrices=False)\n",
    "#     # Truncate to n_components\n",
    "#     S = S_full[:n_components]\n",
    "#     C = Vt[:n_components, :]\n",
    "#     W = U[:, :n_components] * S\n",
    "\n",
    "#     prev_error = cp.inf\n",
    "#     for itr in range(1, max_iter+1):\n",
    "#         # E-step: fill missing values with current reconstruction\n",
    "#         X_hat = cp.dot(W, C)\n",
    "#         X_filled[~mask] = X_hat[~mask]\n",
    "\n",
    "#         # M-step: recompute PCA on filled data\n",
    "#         U, S_full, Vt = cp.linalg.svd(X_filled, full_matrices=False)\n",
    "#         S = S_full[:n_components]\n",
    "#         C = Vt[:n_components, :]\n",
    "#         W = U[:, :n_components] * S\n",
    "\n",
    "#         # Compute reconstruction error on observed entries\n",
    "#         X_hat = cp.dot(W, C)\n",
    "#         err = cp.linalg.norm((X - X_hat)[mask])\n",
    "#         if verbose:\n",
    "#             print(f\"Iteration {itr}: error = {err:.6f}\")\n",
    "\n",
    "#         # Check convergence\n",
    "#         if abs(prev_error - err) < tol:\n",
    "#             if verbose:\n",
    "#                 print(f\"Converged at iteration {itr} with error {err:.6f}\")\n",
    "#             break\n",
    "#         prev_error = err\n",
    "\n",
    "#     new_shape = list(orig_shape)\n",
    "#     new_shape[0] = C.shape[0]\n",
    "#     C = C.reshape(new_shape)\n",
    "#     return W, C, S\n",
    "\n",
    "# #for ii in range(5):\n",
    "# #   weights,components, S = nan_pca(data[sens_id], ii, tol=0.1, verbose=False)\n",
    "# #   print( cp.sqrt(cp.nanmean( (data[sens_id]-cp.tensordot(weights,components,axes=([1], [0])))**2)))\n",
    "# weights,components, S = nan_pca(data[sens_id], 25, tol=0.1, verbose=True)\n",
    "# print( cp.sqrt(cp.nanmean( (data[sens_id]-cp.tensordot(weights,components,axes=([1], [0])))**2)))\n",
    "# plt.figure();\n",
    "# plt.semilogy(S.get());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c9bb2-cfaa-467f-80b9-5b0e4f24d594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nan_pca(mat, n_components, max_iter=50, tol=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform PCA on data with missing values using an alternating least squares approach,\n",
    "    fitting directly on observed entries (unfilled data) and initializing with SVD.\n",
    "\n",
    "    Args:\n",
    "        mat (cp.ndarray): Input data, shape (n_samples, ...). May contain NaNs.\n",
    "        n_components (int): Number of principal components to retain.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Convergence tolerance on reconstruction error change.\n",
    "        verbose (bool): If True, prints iteration diagnostics.\n",
    "\n",
    "    Returns:\n",
    "        W (cp.ndarray): Projected scores (n_samples, n_components).\n",
    "        C (cp.ndarray): Principal components (n_components, n_features).\n",
    "    \"\"\"\n",
    "    # Flatten per-sample features\n",
    "    X = cp.array(mat, copy=True)\n",
    "    n_samples = X.shape[0]\n",
    "    X = X.reshape(n_samples, -1)\n",
    "    mask = ~cp.isnan(X)\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Initialize missing entries to zero for initial SVD\n",
    "    X_mean = X.copy()\n",
    "    X_mean[...] = cp.nanmean(X,0)[None,:]\n",
    "    X_filled = X.copy()\n",
    "    print(X_filled.shape)\n",
    "    X_filled[~mask] = X_mean[~mask]\n",
    "    print(X_filled.shape)\n",
    "    \n",
    "    #X_filled = mat.copy()\n",
    "    #ariel_load.inpaint_along_axis_inplace(X_filled,2)\n",
    "    #X_filled = X_filled.reshape(n_samples,-1)\n",
    "\n",
    "    # Initial SVD on filled data to get starting W and C\n",
    "    U, S_full, Vt = cp.linalg.svd(X_filled, full_matrices=False)\n",
    "    W = U[:, :n_components] * S_full[:n_components]\n",
    "    C = Vt[:n_components, :]\n",
    "\n",
    "    prev_error = cp.inf\n",
    "    # Precompute observed indices\n",
    "    obs_indices = [cp.where(mask[i])[0] for i in range(n_samples)]\n",
    "    feature_indices = [cp.where(mask[:, j])[0] for j in range(n_features)]\n",
    "\n",
    "    for itr in range(1, max_iter + 1):\n",
    "        # Update W: per-sample direct least squares on observed entries of X\n",
    "        for i in range(n_samples):\n",
    "            idx = obs_indices[i]\n",
    "            Ci = C[:, idx]  # (n_components, n_obs)\n",
    "            xi = X[i, idx]  # (n_obs,)\n",
    "            # w_i = (Ci Ci^T)^{-1} Ci xi\n",
    "            W[i] = cp.linalg.solve(Ci @ Ci.T, Ci @ xi)\n",
    "\n",
    "        # Update C: per-feature direct least squares on observed entries of X\n",
    "        for j in range(n_features):\n",
    "            idx = feature_indices[j]\n",
    "            Wj = W[idx]     # (n_obs, n_components)\n",
    "            xj = X[idx, j]  # (n_obs,)\n",
    "            # c_j = (Wj^T Wj)^{-1} Wj^T xj\n",
    "            C[:, j] = cp.linalg.solve(Wj.T @ Wj, Wj.T @ xj)\n",
    "\n",
    "        # Compute reconstruction error on observed entries\n",
    "        X_hat = W @ C\n",
    "        err = cp.linalg.norm((X - X_hat)[mask])\n",
    "        if verbose:\n",
    "            print(f\"Iteration {itr}: error = {err:.6f}\")\n",
    "        if abs(prev_error - err) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {itr} with error {err:.6f}\")\n",
    "            break\n",
    "        prev_error = err\n",
    "\n",
    "    new_shape = list(mat.shape)\n",
    "    new_shape[0] = C.shape[0]\n",
    "    C = C.reshape(new_shape)\n",
    "    return W, C\n",
    "\n",
    "import cupyx.scipy.sparse.linalg as cusparse\n",
    "def nan_pca(mat, n_components, max_iter=50, tol=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform PCA on data with missing values using an alternating least squares approach,\n",
    "    fitting directly on observed entries (unfilled data) with fully GPU-parallel batch solves.\n",
    "\n",
    "    Args:\n",
    "        mat (cp.ndarray): Input data, shape (n_samples, ...). May contain NaNs.\n",
    "        n_components (int): Number of principal components to retain.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Convergence tolerance on reconstruction error change.\n",
    "        verbose (bool): If True, prints iteration diagnostics.\n",
    "\n",
    "    Returns:\n",
    "        W (cp.ndarray): Projected scores (n_samples, n_components).\n",
    "        C (cp.ndarray): Principal components (n_components, n_features).\n",
    "    \"\"\"\n",
    "    # Flatten per-sample features\n",
    "    X = cp.array(mat, copy=True)\n",
    "    n_samples = X.shape[0]\n",
    "    X = X.reshape(n_samples, -1)\n",
    "    mask = (~cp.isnan(X))\n",
    "    n_features = X.shape[1]\n",
    "    #print(cp.sum(cp.sum(mask,1)<n_components))\n",
    "    #print(cp.sum(cp.sum(mask,0)<n_components))\n",
    "    X[cp.sum(mask,1)<n_components+1,:] = 0.\n",
    "    X[:,cp.sum(mask,0)<n_components+1] = 0.\n",
    "    mask = (~cp.isnan(X))\n",
    "    #plt.figure()\n",
    "    #plt.plot(cp.sum(mask,0).get())\n",
    "    X[~mask] = 0.\n",
    "    #X[...] = 0.\n",
    "    \n",
    "    #print(cp.min(cp.sum(~mask,0)), cp.min(cp.sum(~mask,1)))\n",
    "    \n",
    "    \n",
    "\n",
    "#     # Initialize missing entries to zero for initial SVD\n",
    "#     X_mean = X.copy()\n",
    "#     X_mean[...] = cp.nanmean(X,0)[None,:]\n",
    "#     X_filled = X.copy()\n",
    "#     print(X_filled.shape)\n",
    "#     X_filled[~mask] = X_mean[~mask]\n",
    "#     print(X_filled.shape)\n",
    "#     assert not cp.any(cp.isnan(X_filled))\n",
    "#     #print(X.shape)\n",
    "#     #X_filled = copy.deepcopy(mat)\n",
    "#     #ariel_load.inpaint_along_axis_inplace(X_filled, axis=2)\n",
    "#     #X_filled = X_filled.reshape(n_samples, -1)\n",
    "\n",
    "#     # Initial SVD on filled data\n",
    "#     # Initial SVD on filled data (optionally partial)\n",
    "#     # compute top-k singular values/vectors via sparse solver\n",
    "#     U_k, S_k, Vt_k = cusparse.svds(X_filled, k=n_components)\n",
    "#     # svds returns ascending S; reorder to descending\n",
    "#     idx = cp.argsort(S_k)[::-1]\n",
    "#     S = S_k[idx]\n",
    "#     U = U_k[:, idx]\n",
    "#     Vt = Vt_k[idx, :]\n",
    "    \n",
    " \n",
    "#     # else:\n",
    "#     #     U_full, S_full, Vt_full = cp.linalg.svd(X_filled, full_matrices=False)\n",
    "#     #     S = S_full[:n_components]\n",
    "#     #     U = U_full[:, :n_components]\n",
    "#     #     Vt = Vt_full[:n_components, :]\n",
    "    \n",
    "#     # Initialize W and C\n",
    "#     W = U * S\n",
    "#     C = Vt\n",
    "    \n",
    "    rng = cp.random.default_rng(seed=42)\n",
    "    W = rng.standard_normal((n_samples, n_components), dtype=X.dtype)\n",
    "    C = rng.standard_normal((n_components, n_features), dtype=X.dtype)\n",
    "    \n",
    "    mask = mask.astype(X.dtype)  # float mask for weighting\n",
    "\n",
    "    prev_error = cp.inf\n",
    "    for itr in range(1, max_iter + 1):\n",
    "        # === Batch-solve for C: for each j, (W^T M^j W) c_j = W^T (M^j x^j) ===\n",
    "        # Here M^j is mask[:, j] for feature j\n",
    "        # Compute A2_batch[j,p,q] = sum_i mask[i,j] * W[i,p] * W[i,q]\n",
    "        A2_batch = cp.einsum('ip,ij,iq->jpq', W, mask, W)\n",
    "        # Compute B2_batch[j,p] = sum_i mask[i,j] * W[i,p] * X[i,j]\n",
    "        B2_batch = cp.einsum('ip,ij,ij->jp', W, mask, X)\n",
    "        # Solve for C: shape (n_components, n_features)\n",
    "        C = cp.linalg.solve(A2_batch, B2_batch[..., None]).squeeze(-1).T\n",
    "        del A2_batch, B2_batch\n",
    "        C= cp.linalg.qr(C.T)[0].T\n",
    "        #C = C.T\n",
    "        #print(C@C.T)\n",
    "        \n",
    "        # === Batch-solve for W: for each i, (C M_i C^T) w_i = C (M_i x_i) ===\n",
    "        # Compute A_batch[i,a,c] = sum_j mask[i,j] * C[a,j] * C[c,j]\n",
    "        A_batch = cp.einsum('aj,ij,cj->iac', C, mask, C)\n",
    "        # Compute B_batch[i,a] = sum_j mask[i,j] * C[a,j] * X[i,j]\n",
    "        B_batch = cp.einsum('aj,ij,ij->ia', C, mask, X)\n",
    "        \n",
    "        #print(cp.sum(cp.isnan(A_batch)), cp.sum(cp.isnan(B_batch)))\n",
    "        # Solve for W: shape (n_samples, n_components)\n",
    "        # cp.linalg.solve supports batch: A_batch @ W_batch[...,None] = B_batch[...,None]\n",
    "        W = cp.linalg.solve(A_batch, B_batch[..., None]).squeeze(-1)\n",
    "        del A_batch, B_batch\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "              \n",
    "        \n",
    "\n",
    "        # Compute reconstruction and error\n",
    "        err = cp.linalg.norm((X - W @ C) * mask)\n",
    "        if verbose:\n",
    "            print(f\"Iteration {itr}: error = {err:.6f}\")\n",
    "        if abs(prev_error - err) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {itr} with error {err:.6f}\")\n",
    "            break\n",
    "        prev_error = err\n",
    "\n",
    "    new_shape = list(mat.shape)\n",
    "    new_shape[0] = C.shape[0]\n",
    "    C = C.reshape(new_shape)\n",
    "    S_post = cp.linalg.norm(W, axis=0)\n",
    "    order = cp.argsort(S_post)[::-1]\n",
    "    W = W[:, order]\n",
    "    C = C[order,:]\n",
    "    S_post = S_post[order]\n",
    "    return W, C, S_post\n",
    "\n",
    "\n",
    "\n",
    "#    weights,components, S = nan_pca(data[sens_id], ii, tol=0.1, verbose=False)\n",
    "#    print( cp.sqrt(cp.nanmean( (data[sens_id]-cp.tensordot(weights,components,axes=([1], [0])))**2)))\n",
    "weights,components,S = nan_pca(data[sens_id], 25, tol=1e-1, verbose=True)\n",
    "plt.figure();plt.semilogy(S.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1aada-f609-40e4-9775-2cbc6e647b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print( cp.sqrt(cp.nanmean( (data[sens_id]-cp.tensordot(weights,components,axes=([1], [0])))**2)))\n",
    "print( cp.sqrt(cp.nanmean( (data[sens_id]-cp.tensordot(weights,components,axes=([1], [0])))**2)))\n",
    "for ii in range(components.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(components[ii,...].get(), aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(kgs.rms(weights[:,ii]).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384cc85a-5331-4f20-a268-c4891cab4d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(components.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1982f2b-45a8-4a9e-ac46-c872de878cea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2916, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419e2c5-bb9b-4bbf-9104-79f2625da57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
